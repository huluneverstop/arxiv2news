#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è¾“å‡ºæ ¼å¼åŒ–æ¨¡å—
ç”ŸæˆarXivè®ºæ–‡èµ„è®¯çš„æ–‡æ¡£å’Œé…å›¾å½¢å¼è¾“å‡º
"""

import os
import json
from datetime import datetime
from typing import Dict, Any, List
import logging
import re

logger = logging.getLogger(__name__)

class OutputFormatter:
    """è¾“å‡ºæ ¼å¼åŒ–å™¨"""
    
    def __init__(self, timestamp: str, base_output_dir: str = "/media/home/pengyunning/arXiv2xhs/output"):
        self.timestamp = timestamp
        self.base_output_dir =os.path.join(base_output_dir, "output")
    
    def ensure_output_dir(self, output_dir: str):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    def format_output(self, news_content: List[Dict[str, Any]], query: str, papers: List[Dict[str, Any]] = None) -> Dict[str, Any]:
        """æ ¼å¼åŒ–è¾“å‡ºå†…å®¹"""
        try:
            logger.info("å¼€å§‹æ ¼å¼åŒ–è¾“å‡ºå†…å®¹")
            
            # è§£æèµ„è®¯å†…å®¹
            parsed_news = []
            for i, news in enumerate(news_content):
                if news['content'] is None:
                    continue
                parsed = self._parse_news_content(news, papers[i] if papers and i < len(papers) else None)
                if parsed['content']['content_summary'] == 'NOT_PROVIDED':
                    continue
                parsed_news.append(parsed)
    
            
            # ç”Ÿæˆè¾“å‡º
            output = {
                'title': f"arXivæŸ¥è¯¢query - {query}",
                'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'query': query,
                'news_content': parsed_news,
                # 'stats': stats,
                'total_news': len(parsed_news)
            }
            
            logger.info("è¾“å‡ºå†…å®¹æ ¼å¼åŒ–å®Œæˆ")
            return output
            
        except Exception as e:
            logger.error(f"æ ¼å¼åŒ–è¾“å‡ºæ—¶å‡ºé”™: {str(e)}")
            return {}
    
    def _parse_news_content(self, news: Dict[str, Any], paper: Dict[str, Any] = None) -> Dict[str, Any]:
        """è§£æèµ„è®¯å†…å®¹"""
        content = news.get('content', '')
        # è§£ææ¯ä¸ªå†…å®¹ç‰ˆæœ¬
        parsed = self._parse_single_content(content)
        
        # è·å–è®ºæ–‡åŸºæœ¬ä¿¡æ¯
        paper_info = {}
        if paper:
            paper_info = {
                'paper_id': paper.get('id', ''),
                'paper_title': paper.get('title', ''),
                'authors': paper.get('authors', []),
                'arxiv_link': paper.get('links', {}).get('abs', ''),
                'github_link': paper.get('links', {}).get('github', ''),
                'project_link': paper.get('links', {}).get('project', ''),
                'categories': paper.get('categories', []),
                'summary': paper.get('summary', '')
            }
        
        return {
            'paper_info': paper_info,
            'content': parsed,
        }
    
    def _parse_single_content(self, content: str) -> Dict[str, Any]:
        """è§£æå•ä¸ªå†…å®¹ç‰ˆæœ¬"""
        if not content:
            return {
                'title': 'NOT_PROVIDED',
                'alternative_titles': [],
                'content_summary': 'NOT_PROVIDED',
                'tags': []
            }
        
        # æå–æ ‡é¢˜
        title = self._extract_section(content, 'æ ‡é¢˜')
        
        # æå–å¤‡é€‰æ ‡é¢˜
        alternative_titles = self._extract_alternative_titles(content)
        
        # æå–è¯¦ç»†å†…å®¹æ€»ç»“
        content_summary = self._extract_section(content, 'è¯¦ç»†å†…å®¹æ€»ç»“')
        
        # æå–è¯é¢˜æ ‡ç­¾
        tags = self._extract_tags(content)
        
        return {
            'title': title,
            'alternative_titles': alternative_titles,
            'content_summary': content_summary,
            'tags': tags
        }
    
    def _extract_alternative_titles(self, content: str) -> List[str]:
        """æå–å¤‡é€‰æ ‡é¢˜"""
        try:
            # æŸ¥æ‰¾å¤‡é€‰æ ‡é¢˜éƒ¨åˆ†
            start_pattern = "å¤‡é€‰æ ‡é¢˜ï¼š"
            start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                start_pattern = "å¤‡é€‰æ ‡é¢˜:"
                start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                return []
            
            start_pos += len(start_pattern)
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªéƒ¨åˆ†æ ‡é¢˜
            end_pos = len(content)
            for next_section in ['è¯¦ç»†å†…å®¹æ€»ç»“', 'è¯é¢˜æ ‡ç­¾', 'æ ‡é¢˜']:
                if next_section != 'å¤‡é€‰æ ‡é¢˜':
                    next_pos = content.find(f"{next_section}ï¼š", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
                    
                    next_pos = content.find(f"{next_section}:", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
            
            section_content = content[start_pos:end_pos].strip()
            if not section_content:
                return []
            
            # è§£ææ ‡é¢˜åˆ—è¡¨
            titles = []
            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            for separator in [',', 'ï¼Œ', '\n']:
                if separator in section_content:
                    titles = [t.strip() for t in section_content.split(separator) if t.strip()]
                    break
            
            if not titles:
                titles = [section_content.strip()]
            
            return titles
            
        except Exception as e:
            logger.warning(f"æå–å¤‡é€‰æ ‡é¢˜å¤±è´¥: {str(e)}")
            return []
    
    def _extract_tags(self, content: str) -> List[str]:
        """æå–è¯é¢˜æ ‡ç­¾"""
        try:
            # æŸ¥æ‰¾è¯é¢˜æ ‡ç­¾éƒ¨åˆ†
            start_pattern = "è¯é¢˜æ ‡ç­¾ï¼š"
            start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                start_pattern = "è¯é¢˜æ ‡ç­¾:"
                start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                return []
            
            start_pos += len(start_pattern)
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªéƒ¨åˆ†æ ‡é¢˜
            end_pos = len(content)
            for next_section in ['æ ‡é¢˜', 'å¤‡é€‰æ ‡é¢˜', 'è¯¦ç»†å†…å®¹æ€»ç»“']:
                if next_section != 'è¯é¢˜æ ‡ç­¾':
                    next_pos = content.find(f"{next_section}ï¼š", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
                    
                    next_pos = content.find(f"{next_section}:", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
            
            section_content = content[start_pos:end_pos].strip()
            if not section_content:
                return []
            
            # è§£ææ ‡ç­¾åˆ—è¡¨
            tags = []
            # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
            for separator in [',', 'ï¼Œ', '\n']:
                if separator in section_content:
                    tags = [t.strip() for t in section_content.split(separator) if t.strip()]
                    break
            
            if not tags:
                tags = [section_content.strip()]
            
            return tags
            
        except Exception as e:
            logger.warning(f"æå–è¯é¢˜æ ‡ç­¾å¤±è´¥: {str(e)}")
            return []
    
    def _extract_section(self, content: str, section_name: str) -> str:
        """æå–å†…å®¹ä¸­çš„ç‰¹å®šéƒ¨åˆ†"""
        try:
            # æŸ¥æ‰¾éƒ¨åˆ†æ ‡é¢˜
            start_pattern = f"{section_name}ï¼š"
            start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                start_pattern = f"{section_name}:"
                start_pos = content.find(start_pattern)
            
            if start_pos == -1:
                return "NOT_PROVIDED"
            
            start_pos += len(start_pattern)
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªéƒ¨åˆ†æ ‡é¢˜
            end_pos = len(content)
            for next_section in ['æ ‡é¢˜', 'å¤‡é€‰æ ‡é¢˜', 'è¯¦ç»†å†…å®¹æ€»ç»“', 'è¯é¢˜æ ‡ç­¾']:
                if next_section != section_name:
                    next_pos = content.find(f"{next_section}ï¼š", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
                    
                    next_pos = content.find(f"{next_section}:", start_pos)
                    if next_pos != -1 and next_pos < end_pos:
                        end_pos = next_pos
            
            section_content = content[start_pos:end_pos].strip()
            return section_content if section_content else "NOT_PROVIDED"
            
        except Exception as e:
            logger.warning(f"æå–éƒ¨åˆ†å†…å®¹å¤±è´¥: {str(e)}")
            return "NOT_PROVIDED"
    
    def save_output(self, output: Dict[str, Any], query: str) -> List[str]:
        """ä¿å­˜è¾“å‡ºåˆ°æ–‡ä»¶ - æ¯ç¯‡æ–‡ç« åˆ†å¼€å­˜å‚¨"""
        try:
            # åŠ¨æ€æ„å»ºè¾“å‡ºè·¯å¾„
            day_timestamp = datetime.now().strftime("%Y%m%d")
            # day_timestamp = "20250829"
            if query:
                query_dir = query.replace(' ', '_')
                output_dir = os.path.join(self.base_output_dir, day_timestamp, query_dir)
            else:
                output_dir = os.path.join(self.base_output_dir, day_timestamp)
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            self.ensure_output_dir(output_dir)
            
            saved_files = []
            
            # ä¸ºæ¯ç¯‡è®ºæ–‡å•ç‹¬ä¿å­˜æ–‡ä»¶
            news_content = output.get('news_content', [])
            for i, news in enumerate(news_content):
                paper_info = news.get('paper_info', {})
                paper_id = paper_info.get('paper_id', f'paper_{i+1}')
                
                # æ„å»ºå•ç¯‡è®ºæ–‡çš„è¾“å‡ºæ•°æ®
                single_paper_output = {
                    'title': f"arXivè®ºæ–‡èµ„è®¯ - {paper_info.get('paper_title', 'æœªçŸ¥æ ‡é¢˜')}",
                    'generation_time': output.get('generation_time', ''),
                    'query': query,
                    'paper_info': paper_info,
                    'content': news.get('content', {}),
                }
                
                # ç”Ÿæˆæ–‡ä»¶å
                base_filename = f"news_{paper_id}_{self.timestamp}"
                
                # ä¿å­˜JSONæ ¼å¼
                json_path = os.path.join(output_dir, f"{base_filename}.json")
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(single_paper_output, f, ensure_ascii=False, indent=2)
                
                # ä¿å­˜Markdownæ ¼å¼
                md_path = os.path.join(output_dir, f"{base_filename}.md")
                md_content = self._generate_single_paper_markdown(single_paper_output)
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(md_content)
                
                saved_files.append(base_filename)
                logger.info(f"è®ºæ–‡ {paper_id} è¾“å‡ºä¿å­˜å®Œæˆ: {base_filename}")
            
            logger.info(f"æ‰€æœ‰è¾“å‡ºä¿å­˜å®Œæˆï¼Œå…± {len(saved_files)} ä¸ªæ–‡ä»¶")
            return saved_files
            
        except Exception as e:
            logger.error(f"ä¿å­˜è¾“å‡ºæ—¶å‡ºé”™: {str(e)}")
            return []
    
    def _generate_single_paper_markdown(self, output: Dict[str, Any]) -> str:
        """ç”Ÿæˆå•ç¯‡è®ºæ–‡çš„Markdownå†…å®¹"""
        try:
            paper_info = output.get('paper_info', {})
            
            md_content = f"# {output.get('title', '')}\n\n"
            md_content += f"ç”Ÿæˆæ—¶é—´: {output.get('generation_time', '')}\n\n"
            
            # è®ºæ–‡åŸºæœ¬ä¿¡æ¯
            md_content += "## ğŸ“„ è®ºæ–‡ä¿¡æ¯\n\n"
            md_content += f"**è®ºæ–‡ID**: {paper_info.get('paper_id', '')}\n\n"
            md_content += f"**æ ‡é¢˜**: {paper_info.get('paper_title', 'æœªçŸ¥æ ‡é¢˜')}\n\n"
            md_content += f"**ä½œè€…**: {', '.join(paper_info.get('authors', []))}\n\n"
            md_content += f"**arXivé“¾æ¥**: {paper_info.get('arxiv_link', '')}\n\n"
            md_content += f"**GitHubé“¾æ¥**: {paper_info.get('github_link', '')}\n\n"
            md_content += f"**é¡¹ç›®é“¾æ¥**: {paper_info.get('project_link', '')}\n\n"
            md_content += f"**åˆ†ç±»**: {', '.join(paper_info.get('categories', []))}\n\n"
            
            # è®ºæ–‡æ‘˜è¦
            # summary = paper_info.get('summary', '')
            # if summary:
            #     md_content += f"**æ‘˜è¦**:\n{summary}\n\n"
            
            md_content += "---\n\n"
            
            # ä¸‰ä¸ªç‰ˆæœ¬çš„å†…å®¹
            content_data = output.get('content', {})
            if content_data.get('title') != 'NOT_PROVIDED':
                md_content += f"## {content_data.get('title', '')}\n\n"
                
                # å¤‡é€‰æ ‡é¢˜
                alt_titles = content_data.get('alternative_titles', [])
                if alt_titles:
                    md_content += f"**å¤‡é€‰æ ‡é¢˜**: {', '.join(alt_titles)}\n\n"
                
                # å†…å®¹æ€»ç»“
                summary = content_data.get('content_summary', '')
                if summary != 'NOT_PROVIDED':
                    md_content += f"**å†…å®¹æ€»ç»“**:\n{summary}\n\n"
                
                # è¯é¢˜æ ‡ç­¾
                tags = content_data.get('tags', [])
                if tags:
                    md_content += f"**è¯é¢˜æ ‡ç­¾**: {', '.join(tags)}\n\n"
                
                md_content += "---\n\n"
            
            return md_content
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå•ç¯‡è®ºæ–‡Markdownå¤±è´¥: {str(e)}")
            return f"# ç”Ÿæˆå¤±è´¥\n\n{str(e)}"
    