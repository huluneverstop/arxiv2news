#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®ºæ–‡è´¨é‡æ‰“åˆ†æ¨¡å—
ä½¿ç”¨åƒé—®æ¨¡å‹å¯¹arXivè®ºæ–‡è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°
"""

import logging
import asyncio
from typing import Dict, Any, List, Optional
from dashscope import Generation

logger = logging.getLogger(__name__)

class PaperQualityScorer:
    """è®ºæ–‡è´¨é‡æ‰“åˆ†å™¨ - è§„åˆ™å±‚+LLMå±‚æ··åˆè¯„åˆ†"""
    
    def __init__(self, api_key: str, w_rule: float = 0.3, w_llm: float = 0.7):
        self.api_key = api_key
        self.model = "qwen-plus-2025-07-14"  # Qwen3
        # self.w_rule = w_rule  # è§„åˆ™å±‚ é‡
        self.min_score = 6.0
        
        # é¡¶ä¼šåˆ—è¡¨ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
        self.top_conferences = {
            'cs.CV': ['CVPR', 'ICCV', 'ECCV', 'NeurIPS', 'ICML', 'ICLR'],
            'cs.LG': ['NeurIPS', 'ICML', 'ICLR', 'AAAI', 'IJCAI'],
            'cs.CL': ['ACL', 'EMNLP', 'NAACL', 'TACL', 'CL'],
            'cs.AI': ['AAAI', 'IJCAI', 'NeurIPS', 'ICML', 'ICLR'],
            'cs.RO': ['ICRA', 'IROS', 'RSS', 'CoRL'],
            'cs.CR': ['CCS', 'S&P', 'USENIX Security', 'NDSS', 'CRYPTO'],
            'cs.SE': ['ICSE', 'FSE', 'ASE', 'OOPSLA', 'PLDI'],
            'cs.DC': ['OSDI', 'SOSP', 'NSDI', 'FAST', 'ATC'],
            'cs.NI': ['SIGCOMM', 'INFOCOM', 'NSDI', 'IMC'],
            'cs.DS': ['STOC', 'FOCS', 'SODA', 'ICALP', 'ESA']
        }
        
        # è´¨é‡è¯„ä¼°å’Œæ–‡ç« ç±»å‹åˆ†ç±»æç¤ºè¯æ¨¡æ¿
        self.quality_prompt_template = """
            ä½ æ˜¯ä¸€åå­¦æœ¯è®ºæ–‡è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·åŸºäºç»™å®šçš„è®ºæ–‡çš„æœ‰é™ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ‘˜è¦ã€åˆ†ç±»ã€ä½œè€…ã€è¯„è®ºï¼‰ï¼Œå¯¹è®ºæ–‡è¿›è¡Œåˆæ­¥çš„å¤šç»´åº¦è´¨é‡è¯„ä¼°ï¼Œå¹¶åˆ¤æ–­æ–‡ç« ç±»å‹ã€‚  
            æ³¨æ„ï¼šè¾“å…¥ä¿¡æ¯ä»…åŒ…å«æ‘˜è¦ç­‰å…ƒæ•°æ®ï¼Œæ²¡æœ‰å®Œæ•´æ­£æ–‡å’Œå®éªŒç»†èŠ‚ï¼Œè¯·é¿å…è‡†æµ‹ï¼›å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·åœ¨è¯„åˆ†ç†ç”±ä¸­æ˜ç¡®è¯´æ˜"åŸºäºæ‘˜è¦æœ‰é™ä¿¡æ¯çš„æ¨æ–­"ã€‚  

            è®ºæ–‡ä¿¡æ¯ï¼š
            æ ‡é¢˜ï¼š{title}
            æ‘˜è¦ï¼š{summary}
            åˆ†ç±»ï¼š{categories}
            ä½œè€…ï¼š{authors}
            è¯„è®ºï¼š{commment}

            è¯·å®Œæˆä»¥ä¸‹ä¸¤ä¸ªä»»åŠ¡ï¼š

            **ä»»åŠ¡1ï¼šè´¨é‡è¯„ä¼°**
            åŒ…å«ä»¥ä¸‹å››ä¸ªç»´åº¦ï¼Œè¯·å¯¹è®ºæ–‡è¿›è¡Œè¯„åˆ†ï¼ˆ1-10åˆ†ï¼Œ10åˆ†ä¸ºæœ€é«˜åˆ†ï¼‰ï¼š

            1. **æ–°é¢–æ€§ (Novelty)**: ç ”ç©¶æ˜¯å¦æå‡ºäº†æ–°çš„é—®é¢˜ã€æ–¹æ³•æˆ–åº”ç”¨æ–¹å‘ï¼Ÿæ˜¯å¦åœ¨å·²æœ‰å·¥ä½œä¸Šæœ‰æ˜æ˜¾æ”¹è¿›ï¼Ÿ
            2. **ç ”ç©¶å¯é æ€§ (Research Reliability)**: ä»æ‘˜è¦æè¿°åˆ¤æ–­æ–¹æ³•æ˜¯å¦åˆç†ã€æŠ€æœ¯æ€è·¯æ˜¯å¦å¯è¡Œã€é€»è¾‘æ˜¯å¦è‡ªæ´½ã€‚  
            3. **æ½œåœ¨å½±å“åŠ› (Potential Impact)**: ç ”ç©¶æ–¹å‘æ˜¯å¦é‡è¦ï¼Ÿæˆæœæ˜¯å¦æœ‰å¯èƒ½åœ¨å­¦æœ¯ç•Œæˆ–åº”ç”¨é¢†åŸŸäº§ç”Ÿå½±å“ï¼Ÿ
            4. **è¡¨è¾¾ä¸ç»“æ„ (Clarity & Structure)**: æ‘˜è¦æ˜¯å¦å†™ä½œæ¸…æ™°ã€é€»è¾‘è¿è´¯ã€ç»“æ„è§„èŒƒï¼Ÿæ˜¯å¦å­˜åœ¨é€»è¾‘æ¼æ´æˆ–è¡¨è¾¾é—®é¢˜ï¼Ÿ

            | ç»´åº¦ | 9â€“10 åˆ† | 7â€“8 åˆ† | 5â€“6 åˆ† | 1â€“4 åˆ† |
            |------|---------|--------|--------|--------|
            | æ–°é¢–æ€§ | å…·æœ‰é‡å¤§åˆ›æ–°æˆ–çªç ´ï¼Œå¯èƒ½å¼€å¯æ–°æ–¹å‘ | æœ‰ä¸€å®šåˆ›æ–°æ€§æˆ–æ”¹è¿› | ä¸å·²æœ‰å·¥ä½œå·®å¼‚æœ‰é™ | å‡ ä¹æ— åˆ›æ–°ï¼Œé‡å¤å·²æœ‰å·¥ä½œ |
            | ç ”ç©¶å¯é æ€§ | æ–¹æ³•å®Œæ•´ä¸”åˆç†ï¼Œé€»è¾‘ä¸¥è°¨ | æ–¹æ³•åŸºæœ¬åˆç†ï¼Œæœ‰å°ç¼ºå£æˆ–ä¸æ˜ç¡®ä¹‹å¤„ | æŠ€æœ¯åˆç†æ€§ä¸è¶³ï¼Œæè¿°æ¨¡ç³Š | å­˜åœ¨æ˜æ˜¾ä¸å¯é æˆ–ä¸åˆé€»è¾‘çš„åœ°æ–¹ |
            | æ½œåœ¨å½±å“åŠ› | æå…·å½±å“åŠ›ï¼Œå¯èƒ½æ¨åŠ¨é¢†åŸŸå‘å±• | æœ‰ä¸€å®šä»·å€¼ï¼Œå¯èƒ½åœ¨ç‰¹å®šåœºæ™¯åº”ç”¨ | ä»·å€¼æœ‰é™ï¼Œå½±å“è¾ƒå° | åŸºæœ¬æ— æ½œåœ¨å½±å“æˆ–åº”ç”¨æ„ä¹‰ |
            | è¡¨è¾¾ä¸ç»“æ„ | è¡¨è¾¾æ¸…æ™°ï¼Œé€»è¾‘ä¸¥è°¨ï¼Œç»“æ„è§„èŒƒ | å¤§ä½“æ¸…æ™°ï¼Œä½†æœ‰å°‘é‡é—®é¢˜ | è¡¨è¾¾ä¸€èˆ¬ï¼Œç»“æ„ä¸å¤Ÿç´§å‡‘ | è¡¨è¾¾æ··ä¹±æˆ–é€»è¾‘æ€§å·® |

            **ä»»åŠ¡2ï¼šæ–‡ç« ç±»å‹åˆ†ç±»**
            è¯·æ ¹æ®è®ºæ–‡ç‰¹å¾åˆ¤æ–­æ–‡ç« å±äºä»¥ä¸‹å“ªç§ç±»å‹ï¼Œå‡†ç¡®è¾“å‡ºç±»å‹å…³é”®è¯ï¼Œä¸è¦è¾“å‡ºå…¶ä»–å†…å®¹:

            ç»¼è¿°å‹ï¼šsurvey
            æ–°æ–¹æ³•å‹ï¼šmethod
            
            è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºç»“æœï¼š

            {{
                "novelty": 8,
                "research_reliability": 7,
                "potential_impact": 6,
                "clarit_structure": 8,
                "overall_score": 7.25,
                "paper_type": "method",
                "paper_type_reason": "è®ºæ–‡æå‡ºäº†æ–°çš„ç®—æ³•æ¨¡å‹ï¼Œå®éªŒè®¾è®¡ä¸¥è°¨",
                "reasoning": {{
                    "novelty_reason": "è®ºæ–‡æå‡ºäº†æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨ç°æœ‰æ–¹æ³•åŸºç¡€ä¸Šæœ‰æ‰€åˆ›æ–°",
                    "research_reliability_reason": "æŠ€æœ¯æ–¹æ¡ˆåˆç†ï¼Œå®éªŒè®¾è®¡è¾ƒä¸ºå®Œæ•´ï¼Œä½†ç¼ºä¹ä¸æ›´å¤šbaselineçš„å¯¹æ¯”",
                    "potential_impact_reason": "åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸæœ‰æ½œåœ¨åº”ç”¨ä»·å€¼ï¼Œä½†å®é™…éƒ¨ç½²å¯èƒ½é¢ä¸´æŒ‘æˆ˜",ğŸ˜Ÿ
                    "clarit_structure_reason": "å†™ä½œæ¸…æ™°ï¼Œé€»è¾‘ç»“æ„åˆç†ï¼Œå®éªŒæ•°æ®å……åˆ†ï¼Œæ— æ˜æ˜¾æ°´æ–‡ç‰¹å¾"
                }},
                "confidence": 0.85
            }}
             """

    def _categorize_url(self, url: str) -> str:
        """
        å¯¹URLè¿›è¡Œåˆ†ç±»
        
        Args:
            url: è¦åˆ†ç±»çš„URL
            
        Returns:
            åˆ†ç±»ç»“æœ: 'github', 'project', 'other'
        """
        url_lower = url.lower()
        
        # GitHubé“¾æ¥
        if 'github.com' in url_lower:
            return 'github'
        else:
            return 'project'

    def _rule_filter(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§„åˆ™å±‚ç­›é€‰ï¼šæ£€æŸ¥è®ºæ–‡æ˜¯å¦æ»¡è¶³è¿›å…¥LLMå±‚çš„åŸºæœ¬æ¡ä»¶
        æ¡ä»¶ï¼šæ˜¯é¡¶ä¼š OR æœ‰é¡¹ç›®/githubé“¾æ¥ï¼ˆè‡³å°‘æ»¡è¶³ä¸€ä¸ªï¼‰
        è¿”å›: {"passed": bool, "details": {...}}
        """
        import re
        
        details = {
            "is_top_conference": False,
            "has_links": False,
            "conference_name": None
        }
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºé¡¶ä¼š
        journal_ref = paper.get('journal_ref', '')
        comment = paper.get('comment', '')
        publication_text = f"{journal_ref} {comment}".upper()
        paper_categories = paper.get('categories', [])
        
        for category in paper_categories:
            if category in self.top_conferences:
                for conf in self.top_conferences[category]:
                    if conf.upper() in publication_text:
                        details["is_top_conference"] = True
                        details["conference_name"] = conf
                        break
                if details["is_top_conference"]:
                    break
        
        # 2. æ£€æŸ¥æ˜¯å¦æœ‰é¡¹ç›®/githubé“¾æ¥
        abstract = paper.get('summary', '')
        full_text = f"{abstract} {comment}"
        
        # ä½¿ç”¨æä¾›çš„é“¾æ¥æ£€æµ‹æ­£åˆ™è¡¨è¾¾å¼
        link_pattern = re.compile(r'https?://[^\s]+', re.IGNORECASE)
        link_matches = link_pattern.findall(full_text)
        
        # å¯¹æ‰¾åˆ°çš„é“¾æ¥è¿›è¡Œåˆ†ç±»
        if link_matches:
            details["has_links"] = True
            
            # å¯¹æ¯ä¸ªé“¾æ¥è¿›è¡Œåˆ†ç±»
            for link in link_matches:
                link_type = self._categorize_url(link)
                if link_type == 'github':
                    paper['links']['github']=link
                else:
                    paper['links']['project']=link

        # åˆ¤æ–­æ˜¯å¦é€šè¿‡ç­›é€‰
        passed = details["is_top_conference"] or details["has_links"]
        
        return {
            "passed": passed,
            "details": details
        }

    async def llm_filter(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """
        LLMå±‚è¯„åˆ†ï¼šåŸºäºæ–°é¢–æ€§ã€æŠ€æœ¯æ·±åº¦ã€åº”ç”¨ä»·å€¼ã€é¢†åŸŸè´¡çŒ®ï¼ŒåŒæ—¶åˆ¤æ–­æ–‡ç« ç±»å‹
        è¿”å›: {"llm_score": 0-10, "paper_type": "A/B/C", "details": {...}}
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self.quality_prompt_template.format(
                title=paper.get('title', ''),
                summary=paper.get('summary', ''),
                categories=', '.join(paper.get('categories', [])),
                authors=', '.join(paper.get('authors', [])),
                commment=paper.get('comment', '')
            )
            
            # è°ƒç”¨åƒé—®API
            response = await self._call_qwen_api(prompt)
            
            # è§£æJSONå“åº”
            llm_result = self._parse_score_response(response) 
            
            return {
                "llm_score": llm_result.get('overall_score', 0.0),
                "llm_details": llm_result,
                "paper_type": llm_result.get('paper_type', 'method'),
                "paper_type_reason": llm_result.get('paper_type_reason', 'é»˜è®¤ä¸ºmethod'),
            }
            
        except Exception as e:
            logger.error(f"LLMè¯„åˆ†å¤±è´¥: {str(e)}")
            return {
                "llm_score": 0.0,
                "llm_details": self._get_default_score(paper),
                "paper_type": "method",
                "paper_type_reason": "å‡ºç°é”™è¯¯ï¼Œé»˜è®¤ä¸ºmethod",
            }

    async def _score_paper(self, paper: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹å•ç¯‡è®ºæ–‡è¿›è¡Œæ··åˆè´¨é‡è¯„åˆ†ï¼ˆè§„åˆ™å±‚ç­›é€‰+è§„åˆ™å±‚è¯„åˆ†+LLMå±‚ï¼‰"""
        try:
            logger.info(f"å¼€å§‹è¯„ä¼°è®ºæ–‡è´¨é‡: {paper.get('id', 'unknown')}")
            
            # 1. è§„åˆ™å±‚ç­›é€‰
            filter_result = self._rule_filter(paper)
            if not filter_result["passed"]:
                logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} æœªé€šè¿‡è§„åˆ™å±‚ç­›é€‰ï¼Œè·³è¿‡LLMå±‚è¯„åˆ†")
                return {
                    "paper_id": paper.get('id', ''),
                    "paper_title": paper.get('title', ''),
                    "rule_passed": filter_result["passed"],
                    "rule_details": filter_result["details"],
                    "llm_score": 0.0,
                    "llm_details": {}

                }
            
            # 2. LLMå±‚è¯„åˆ†ï¼ˆåŒ…å«æ–‡ç« ç±»å‹åˆ¤æ–­ï¼‰
            llm_result = await self.llm_filter(paper)
            llm_score = llm_result["llm_score"]
            llm_details = llm_result["llm_details"]
            paper_type = llm_result["paper_type"]
            paper_type_reason = llm_result["paper_type_reason"]
        
            # 3. ç»„åˆç»“æœ
            score_result = {
                "paper_id": paper.get('id', ''),
                "paper_title": paper.get('title', ''),
                "rule_passed": filter_result["passed"],
                "rule_details": filter_result["details"],
                "rule_score": 1.0 if filter_result["passed"] else 0.0,  # è§„åˆ™å±‚é€šè¿‡ä¸º1.0ï¼Œå¦åˆ™ä¸º0.0
                "llm_score": llm_score,
                "llm_details": llm_details,
                "paper_type": paper_type,
                "paper_type_reason": paper_type_reason,
            }
            paper['paper_type'] = paper_type

            
            logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} è´¨é‡è¯„ä¼°å®Œæˆ - å¾—åˆ†: {llm_score:.2f}, ç±»å‹: {paper_type}")
            return score_result
            
        except Exception as e:
            logger.error(f"è¯„ä¼°è®ºæ–‡è´¨é‡æ—¶å‡ºé”™: {str(e)}")
            return {
                    "paper_id": paper.get('id', ''),
                    "paper_title": paper.get('title', ''),
                    "rule_passed": filter_result["passed"],
                    "rule_details": filter_result["details"],
                    "rule_score": 1.0 if filter_result["passed"] else 0.0,
                    "llm_score": 0.0,
                    "llm_details": {},
                    "paper_type": "method",
                    "paper_type_reason": "å‡ºç°é”™è¯¯ï¼Œé»˜è®¤ä¸ºmethod",

                }

    
    async def batch_score_papers(self, papers: List[Dict[str, Any]]) -> Dict[str, Any]:
        """æ‰¹é‡è¯„ä¼°è®ºæ–‡è´¨é‡å¹¶è¿‡æ»¤ä½è´¨é‡è®ºæ–‡"""
        try:
            logger.info(f"å¼€å§‹æ‰¹é‡è¯„ä¼° {len(papers)} ç¯‡è®ºæ–‡çš„è´¨é‡")
            
            scored_papers = []
            rule_filtered_count = 0
            score_filtered_count = 0
            total_processed = 0
            
            for i, paper in enumerate(papers):
                logger.info(f"æ­£åœ¨è¯„ä¼°ç¬¬ {i+1}/{len(papers)} ç¯‡è®ºæ–‡...")
                
                # è¯„ä¼°å•ç¯‡è®ºæ–‡
                score_result = await self._score_paper(paper)
                total_processed += 1
                
                # æ£€æŸ¥æ˜¯å¦è¢«è§„åˆ™å±‚ç­›é€‰æ‰
                rule_passed = score_result.get('rule_passed', False)
                if rule_passed:
                    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€ä½åˆ†æ•°è¦æ±‚
                    logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} é€šè¿‡è§„åˆ™å±‚ç­›é€‰")
                    total_score = score_result.get('llm_score', 0)
                    if total_score >= self.min_score:
                        scored_papers.append({
                            'paper': paper,
                            'quality_score': score_result
                        })
                        logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} é€šè¿‡è´¨é‡ç­›é€‰ (æ€»åˆ†: {total_score:.2f})")
                    else:
                        score_filtered_count += 1
                        logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} æœªé€šè¿‡è´¨é‡ç­›é€‰ (æ€»åˆ†: {total_score:.2f})")
                
                else:
                    rule_filtered_count += 1
                    logger.info(f"è®ºæ–‡ {paper.get('id', 'unknown')} æœªé€šè¿‡è§„åˆ™å±‚ç­›é€‰")
                    continue
                
                
                # æ·»åŠ å»¶è¿Ÿé¿å…APIé™åˆ¶
                await asyncio.sleep(1)
            
            logger.info(f"æ‰¹é‡è¯„ä¼°å®Œæˆï¼Œé€šè¿‡ç­›é€‰: {len(scored_papers)} ç¯‡ï¼Œè§„åˆ™å±‚è¿‡æ»¤: {rule_filtered_count} ç¯‡ï¼Œåˆ†æ•°è¿‡æ»¤: {score_filtered_count} ç¯‡")
            
            return {
                'scored_papers': scored_papers,
                'statistics': {
                    'total_processed': total_processed,
                    'rule_filtered': rule_filtered_count,
                    'score_filtered': score_filtered_count,
                    'passed': len(scored_papers),
                    'rule_filter_rate': rule_filtered_count / total_processed if total_processed > 0 else 0,
                    'score_filter_rate': score_filtered_count / total_processed if total_processed > 0 else 0,
                    'final_pass_rate': len(scored_papers) / total_processed if total_processed > 0 else 0
                }
            }
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è¯„ä¼°è®ºæ–‡è´¨é‡æ—¶å‡ºé”™: {str(e)}")
            return {
                'scored_papers': [],
                'statistics': {
                    'total_processed': 0,
                    'rule_filtered': 0,
                    'score_filtered': 0,
                    'passed': 0,
                    'rule_filter_rate': 0,
                    'score_filter_rate': 0,
                    'final_pass_rate': 0
                }
            }
    
    def _parse_score_response(self, response: str) -> Dict[str, Any]:
        """è§£æåƒé—®APIçš„JSONå“åº”"""
        try:
            import json
            import re
            
            # å°è¯•ç›´æ¥è§£æJSON
            try:
                return json.loads(response)
            except json.JSONDecodeError:
                pass
            
            # å¦‚æœç›´æ¥è§£æå¤±è´¥ï¼Œå°è¯•æå–JSONéƒ¨åˆ†
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            
            # å¦‚æœéƒ½å¤±è´¥ï¼Œè¿”å›é»˜è®¤è¯„åˆ†
            logger.warning("æ— æ³•è§£æAPIå“åº”ä¸ºJSONï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†")
            return self._get_default_score()
            
        except Exception as e:
            logger.error(f"è§£æè¯„åˆ†å“åº”å¤±è´¥: {str(e)}")
            return self._get_default_score()
    
    def _get_default_score(self, paper: Dict[str, Any] = None) -> Dict[str, Any]:
        """è·å–é»˜è®¤è¯„åˆ†ï¼ˆå½“APIè°ƒç”¨å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        if paper:
            
            return {
                    "novelty": 0,
                    "research_reliability": 0,
                    "potential_impact": 0,
                    "clarit_structure": 0,
                    "overall_score": 0.0,
                    "paper_type": "method",
                    "paper_type_reason": "é»˜è®¤ä¸ºmethod",
                    "reasoning": {
                        "novelty_reason": "æ— æ³•è¯„ä¼°ï¼ŒAPIè°ƒç”¨å¤±è´¥",
                        "research_reliability_reason": "æ— æ³•è¯„ä¼°ï¼ŒAPIè°ƒç”¨å¤±è´¥",
                        "potential_impact_reason": "æ— æ³•è¯„ä¼°ï¼ŒAPIè°ƒç”¨å¤±è´¥",
                        "clarit_structure_reason": "æ— æ³•è¯„ä¼°ï¼ŒAPIè°ƒç”¨å¤±è´¥"
                    },
                    "confidence": 10.0
                }
        else:
            return {
                    "novelty": 0,
                    "research_reliability": 0,
                    "potential_impact": 0,
                    "clarit_structure": 0,
                    "overall_score": 0.0,
                    "paper_type": "method",
                    "paper_type_reason": "é»˜è®¤ä¸ºmethod",
                    "reasoning": {
                        "novelty_reason": "æ— æ³•è¯„ä¼°ï¼Œä¸å­˜åœ¨è®ºæ–‡",
                        "research_reliability_reason": "æ— æ³•è¯„ä¼°ï¼Œä¸å­˜åœ¨è®ºæ–‡",
                        "potential_impact_reason": "æ— æ³•è¯„ä¼°ï¼Œä¸å­˜åœ¨è®ºæ–‡",
                        "clarit_structure_reason": "æ— æ³•è¯„ä¼°ï¼Œä¸å­˜åœ¨è®ºæ–‡"
                    },
                    "confidence": 10.0
                }
    
    async def _call_qwen_api(self, prompt: str) -> str:
        """è°ƒç”¨åƒé—®API"""
        try:
            # ä½¿ç”¨å¼‚æ­¥æ–¹å¼è°ƒç”¨åƒé—®API
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                self._sync_call_qwen_api, 
                prompt
            )
            return response
            
        except Exception as e:
            logger.error(f"è°ƒç”¨åƒé—®APIå¤±è´¥: {str(e)}")
            raise e
    
    def _sync_call_qwen_api(self, prompt: str) -> str:
        """åŒæ­¥è°ƒç”¨åƒé—®API"""
        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                api_key=self.api_key,
                max_tokens=2000,
                temperature=0.3  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„è¯„åˆ†
            )
            
            if response.status_code == 200:
                return response.output.text
            else:
                raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.message}")
                
        except Exception as e:
            logger.error(f"åƒé—®APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise e
    
    def generate_quality_report(self, batch_result: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè´¨é‡è¯„ä¼°æŠ¥å‘Š"""
        try:
            scored_papers = batch_result.get('scored_papers', [])
            statistics = batch_result.get('statistics', {})
            
            if not scored_papers:
                return {
                    "filter_statistics": {
                        "passed": 0,
                        "rule_filtered": statistics.get('rule_filtered', 0),
                        "score_filtered": statistics.get('score_filtered', 0)
                    },
                    "paper_type_counts": {
                        "method": 0,
                        "survey": 0
                    },
                    "papers": []
                }
            
            # 1. ç­›é€‰ç»Ÿè®¡
            filter_stats = {
                "passed": len(scored_papers),
                "rule_filtered": statistics.get('rule_filtered', 0),
                "score_filtered": statistics.get('score_filtered', 0)
            }
            
            # 2. æ–‡ç« ç±»å‹ç»Ÿè®¡
            paper_types = [item['quality_score'].get('paper_type', 'method') for item in scored_papers]
            type_counts = {
                'method': paper_types.count('method'),
                'survey': paper_types.count('survey')
            }
            
            # 3. æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯
            papers_detail = []
            for item in scored_papers:
                paper_info = {
                    "id": item['paper']['id'],
                    "title": item['paper']['title'],
                    "rule_score": item['quality_score']['rule_score'],
                    "llm_score": item['quality_score']['llm_score'],
                    "paper_type": item['quality_score'].get('paper_type', 'method'),
                    "rule_details": item['quality_score'].get('rule_details', {}),
                    "llm_details": item['quality_score'].get('llm_details', {})
                }
                papers_detail.append(paper_info)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = {
                "filter_statistics": filter_stats,
                "paper_type_counts": type_counts,
                "papers": papers_detail
            }
            
            return report
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè´¨é‡æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return {"error": str(e)}
